{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "Convolutional Neural Networks (CNN) is Another deep learning architecture that is employed for hierarchical document classification. Although originally built for image processing with architecture similar to the visual cortex, CNNs have also been effectively used for text classification. In a basic CNN for image processing, an image tensor is convolved with a set of kernels of size d by d. These convolution layers are called feature maps and can be stacked to provide multiple filters on the input. To reduce the computational complexity, CNNs use pooling which reduces the size of the output from one layer to the next in the network. Different pooling techniques are used to reduce outputs while preserving important features.\n",
    "The most common pooling method is max pooling where the maximum element is selected from the pooling window. In order to feed the pooled output from stacked featured maps to the next layer, the maps are flattened into one column. The final layers in a CNN are typically fully connected dense layers. In general, during the back-propagation step of a convolutional neural network not only the weights are adjusted but also the feature detector filters. A potential problem of CNN used for text is the number of ‘channels’, Sigma (size of the feature space). This might be very large (e.g. 50K), for text but for images this is less of a problem (e.g. only 3 channels of RGB). This means the dimensionality of the CNN for text is very high.\n",
    "\n",
    "<img src=\"./images/CNN.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text Using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a data tokenizer\n",
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=179210,\\\n",
    "                       MAX_SEQUENCE_LENGTH=500):\n",
    "    '''\n",
    "    The function takes Train and Test datasets with text.\n",
    "    Converts them into tokens, and returns tokenized version of\n",
    "    both the sets, and the embedding matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : list with each item having a set of words that will be used\n",
    "    for training the model \n",
    "    X_test : list with each item having a set of words that will be used\n",
    "    for testing the model\n",
    "    MAX_NB_WORDS : Number of maximum words to be added in the tokenizer \n",
    "    vocabulary\n",
    "    MAX_SEQUENCE_LENGTH : Maximum length of sentences in the \n",
    "    '''\n",
    "    # set a random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "    \n",
    "    # concatenate train and text to build a combined vocabulary\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    \n",
    "    # initiate tokenizer\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    \n",
    "    # fit tokenizer on texts\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    \n",
    "    # build sequences\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    # dictionary for total vocabulary\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # pad sequences from left to make them of equal lengths\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # total unique words in vocab\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    # split tokenized text into train and test sets\n",
    "    indices = np.arange(text.shape[0])\n",
    "    text = text[indices]\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    \n",
    "    # create embedding using GLOVE\n",
    "    embeddings_index = {}\n",
    "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # print total words in embedding\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    # return train, test, vocabulary and embedding details\n",
    "    return (X_train, X_test, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model class using Pytorch Lightning\n",
    "\n",
    "Pytorch Lightning provides a standard wrapper to load data, define and train deep learning models.\n",
    "\n",
    "In this codeblock we define:\n",
    "1. Model\n",
    "2. Training/Validation/Test Steps\n",
    "3. Optimizer settings\n",
    "4. Train/Validation/Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "class CoolSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embedding_matrix, nclasses):\n",
    "        '''\n",
    "        Convolution neural network architectures.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: the dimensions of input layer\n",
    "        nclasses: the dimensions of output layer\n",
    "        dropout: the probability of dropping out.\n",
    "        '''\n",
    "        super(CoolSystem, self).__init__()\n",
    "        \n",
    "        self.nclasses = nclasses\n",
    "        \n",
    "        ## Embedding Layer, Add parameter \n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], \\\n",
    "                                      embedding_matrix.shape[1]) \n",
    "        et = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        self.embedding.weight = nn.Parameter(et)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Conv1d(50, 128, kernel_size = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5)\n",
    "        )\n",
    "        self.feature2 = nn.Sequential(\n",
    "            nn.Conv1d(50, 128, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5)\n",
    "        )\n",
    "        self.feature3 = nn.Sequential(\n",
    "            nn.Conv1d(50, 128, kernel_size = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5)\n",
    "        )\n",
    "        self.feature4 = nn.Sequential(\n",
    "            nn.Conv1d(50, 128, kernel_size = 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5)\n",
    "        )\n",
    "        self.feature5 = nn.Sequential(\n",
    "            nn.Conv1d(50, 128, kernel_size = 6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5)\n",
    "        )\n",
    "        self.feature6 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.MaxPool1d(5),\n",
    "            nn.Conv1d(128,128,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.MaxPool1d(30),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, nclasses))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Passes the input through Deep neural network defined before.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: input\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        f1 = self.feature1(x)\n",
    "        f2 = self.feature2(x)\n",
    "        f3 = self.feature3(x)\n",
    "        f4 = self.feature4(x)\n",
    "        f5 = self.feature5(x)\n",
    "        x = torch.cat([f1,f2,f3,f4,f5], dim=2)\n",
    "        x = self.feature6(x)\n",
    "                \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward \n",
    "        through network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward\n",
    "        through trained network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        '''\n",
    "        Takes and stacks validation loss.\n",
    "        Early stop can also be defined here\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Outputs: Output of validation step\n",
    "        '''\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Optimizer for the network\n",
    "\n",
    "        '''\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    @pl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        '''\n",
    "        Training data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_train_Glove),\\\n",
    "                                        torch.LongTensor(y_train)), \\\n",
    "               batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        '''\n",
    "        Validation data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_test_Glove),\\\n",
    "                                        torch.LongTensor(y_test)), \\\n",
    "               batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        '''\n",
    "        Test data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_test_Glove),\\\n",
    "                                        torch.LongTensor(y_test)), \\\n",
    "               batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text dataset (20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n",
      "Total 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Load test data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# make x and y\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# tokenize text and obtain embedding matrix\n",
    "X_train_Glove,X_test_Glove, embedding_matrix = loadData_Tokenizer(X_train,\\\n",
    "                                                                  X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model using Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False, used: False\n",
      "           Name        Type   Params\n",
      "0     embedding   Embedding  8960500\n",
      "1      feature1  Sequential    12928\n",
      "2    feature1.0      Conv1d    12928\n",
      "3    feature1.1        ReLU        0\n",
      "4    feature1.2   MaxPool1d        0\n",
      "5      feature2  Sequential    19328\n",
      "6    feature2.0      Conv1d    19328\n",
      "7    feature2.1        ReLU        0\n",
      "8    feature2.2   MaxPool1d        0\n",
      "9      feature3  Sequential    25728\n",
      "10   feature3.0      Conv1d    25728\n",
      "11   feature3.1        ReLU        0\n",
      "12   feature3.2   MaxPool1d        0\n",
      "13     feature4  Sequential    32128\n",
      "14   feature4.0      Conv1d    32128\n",
      "15   feature4.1        ReLU        0\n",
      "16   feature4.2   MaxPool1d        0\n",
      "17     feature5  Sequential    38528\n",
      "18   feature5.0      Conv1d    38528\n",
      "19   feature5.1        ReLU        0\n",
      "20   feature5.2   MaxPool1d        0\n",
      "21     feature6  Sequential  1093396\n",
      "22   feature6.0      Conv1d    82048\n",
      "23   feature6.1        ReLU        0\n",
      "24   feature6.2     Dropout        0\n",
      "25   feature6.3   MaxPool1d        0\n",
      "26   feature6.4      Conv1d    82048\n",
      "27   feature6.5        ReLU        0\n",
      "28   feature6.6     Dropout        0\n",
      "29   feature6.7   MaxPool1d        0\n",
      "30   feature6.8     Flatten        0\n",
      "31   feature6.9      Linear   394240\n",
      "32  feature6.10        ReLU        0\n",
      "33  feature6.11     Dropout        0\n",
      "34  feature6.12      Linear   524800\n",
      "35  feature6.13        ReLU        0\n",
      "36  feature6.14     Dropout        0\n",
      "37  feature6.15      Linear    10260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [03:25<00:00,  1.50it/s, avg_val_loss=1.1, batch_nb=88, epoch=14, loss=0.396] "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = CoolSystem(embedding_matrix, 20)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer(max_nb_epochs=15)  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.53      0.57       319\n",
      "           1       0.63      0.56      0.59       389\n",
      "           2       0.78      0.33      0.46       394\n",
      "           3       0.37      0.19      0.25       392\n",
      "           4       0.36      0.59      0.45       385\n",
      "           5       0.85      0.50      0.63       395\n",
      "           6       0.87      0.69      0.77       390\n",
      "           7       0.76      0.77      0.77       396\n",
      "           8       0.82      0.82      0.82       398\n",
      "           9       0.94      0.84      0.89       397\n",
      "          10       0.96      0.93      0.94       399\n",
      "          11       0.89      0.63      0.74       396\n",
      "          12       0.34      0.79      0.48       393\n",
      "          13       0.78      0.84      0.81       396\n",
      "          14       0.68      0.87      0.76       394\n",
      "          15       0.84      0.67      0.74       398\n",
      "          16       0.55      0.72      0.62       364\n",
      "          17       0.97      0.68      0.80       376\n",
      "          18       0.43      0.59      0.50       310\n",
      "          19       0.34      0.31      0.33       251\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      7532\n",
      "   macro avg       0.69      0.64      0.65      7532\n",
      "weighted avg       0.70      0.65      0.66      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        predicted = model.forward(torch.LongTensor(X_test_Glove))\n",
    "\n",
    "# get classification report\n",
    "predicted = predicted.detach().numpy()\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
