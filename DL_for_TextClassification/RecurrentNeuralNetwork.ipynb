{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "Recurrent Neural Networks (RNN) is another neural network architecture that is addressed by the researchers for text mining and classification. RNN assigns more weights to the previous data points of sequence. Therefore, this technique is a powerful method for text, string, and sequential data classification. In RNN, the neural net considers the information of previous nodes in a very sophisticated method which allows for better semantic analysis of the structures in the dataset.\n",
    "<img src=\"./images/RNN.png\">\n",
    "\n",
    "## Gated Recurrent Unit (GRU)\n",
    "Gated Recurrent Unit (GRU) is a gating mechanism for RNN which was introduced by J. Chung et al. and K.Cho et al.. GRU is a simplified variant of the LSTM architecture, but there are differences as follows: GRU contains two gates and does not possess any internal memory (as shown in Figure; and finally, a second non-linearity is not applied (tanh in Figure).\n",
    "<img src=\"./images/GRU.png\">\n",
    "\n",
    "## Long Short-Term Memory (LSTM)\n",
    "Long Short-Term Memory~(LSTM) was introduced by S. Hochreiter and J. Schmidhuber and developed by many research scientists.\n",
    "To deal with these problems Long Short-Term Memory (LSTM) is a special type of RNN that preserves long term dependency in a more effective way compared to the basic RNNs. This is particularly useful to overcome vanishing gradient problem as LSTM uses multiple gates to carefully regulate the amount of information that will be allowed into each node state. The figure shows the basic cell of a LSTM model.\n",
    "<img src=\"./images/LSTM.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a data tokenizer\n",
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=179210,MAX_SEQUENCE_LENGTH=500):\n",
    "    '''\n",
    "    The function takes Train and Test datasets with text.\n",
    "    Converts them into tokens, and returns tokenized version of\n",
    "    both the sets, and the embedding matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : list with each item having a set of words that will be used\n",
    "    for training the model \n",
    "    X_test : list with each item having a set of words that will be used\n",
    "    for testing the model\n",
    "    MAX_NB_WORDS : Number of maximum words to be added in the tokenizer \n",
    "    vocabulary\n",
    "    MAX_SEQUENCE_LENGTH : Maximum length of sentences in the \n",
    "    '''\n",
    "    # set a random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "    \n",
    "    # concatenate train and text to build a combined vocabulary\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    \n",
    "    # initiate tokenizer\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    \n",
    "    # fit tokenizer on texts\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    \n",
    "    # build sequences\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    # dictionary for total vocabulary\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # pad sequences from left to make them of equal lengths\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # total unique words in vocab\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    # split tokenized text into train and test sets\n",
    "    indices = np.arange(text.shape[0])\n",
    "    text = text[indices]\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    \n",
    "    # create embedding using GLOVE\n",
    "    embeddings_index = {}\n",
    "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # print total words in embedding\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    # return train, test, vocabulary and embedding details\n",
    "    return (X_train, X_test, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model class using Pytorch Lightning\n",
    "\n",
    "Pytorch Lightning provides a standard wrapper to load data, define and train deep learning models.\n",
    "\n",
    "In this codeblock we define:\n",
    "1. Model\n",
    "2. Training/Validation/Test Steps\n",
    "3. Optimizer settings\n",
    "4. Train/Validation/Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "class CoolSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embedding_matrix, nclasses):\n",
    "        '''\n",
    "        Recurrent Neural Network Architectures.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: the dimensions of input layer\n",
    "        nclasses: the dimensions of output layer\n",
    "        dropout: the probability of dropping out.\n",
    "        '''\n",
    "        super(CoolSystem, self).__init__()\n",
    "        \n",
    "        self.nclasses = nclasses\n",
    "        \n",
    "        ## Embedding Layer, Add parameter \n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1]) \n",
    "        et = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        self.embedding.weight = nn.Parameter(et)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru1 = nn.GRU(50, 256, num_layers =1, dropout = 0.2)\n",
    "        self.dp1 = nn.Dropout(p = 0.5)\n",
    "        self.gru2 = nn.GRU(256, 256, num_layers =1, dropout = 0.2)\n",
    "        self.dp2 = nn.Dropout(p = 0.5)\n",
    "        self.gru3 = nn.GRU(256, 256, num_layers =1, dropout = 0.2)\n",
    "        self.dp3 = nn.Dropout(p = 0.5)\n",
    "        self.gru4 = nn.GRU(256, 256, num_layers =1, dropout = 0.2)\n",
    "        self.l1 =  nn.Linear(256, nclasses)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Passes the input through Deep neural network defined before.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: input\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dp1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dp2(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dp3(x)\n",
    "        x, _ = self.gru4(x)\n",
    "        x, _ = torch.max(x, 0)  \n",
    "        x = self.l1(x)\n",
    "                \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward through network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward through trained network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        '''\n",
    "        Takes and stacks validation loss.\n",
    "        Early stop can also be defined here\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Outputs: Output of validation step\n",
    "        '''\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Optimizer for the network\n",
    "\n",
    "        '''\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    @pl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        '''\n",
    "        Training data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_train_Glove), torch.LongTensor(y_train)), batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        '''\n",
    "        Validation data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_test_Glove), torch.LongTensor(y_test)), batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        '''\n",
    "        Test data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_test_Glove), torch.LongTensor(y_test)), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text dataset (20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n",
      "Total 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Load test data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# make x and y\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# tokenize text and obtain embedding matrix\n",
    "X_train_Glove,X_test_Glove, embedding_matrix = loadData_Tokenizer(X_train,\\\n",
    "                                                                  X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model using Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skp454/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False, used: False\n",
      "        Name       Type   Params\n",
      "0  embedding  Embedding  8960500\n",
      "1       gru1        GRU   236544\n",
      "2        dp1    Dropout        0\n",
      "3       gru2        GRU   394752\n",
      "4        dp2    Dropout        0\n",
      "5       gru3        GRU   394752\n",
      "6        dp3    Dropout        0\n",
      "7       gru4        GRU   394752\n",
      "8         l1     Linear     5140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [20:56<00:00,  3.22s/it, avg_val_loss=1.19, batch_nb=88, epoch=19, loss=0.206]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = CoolSystem(embedding_matrix, 20)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer(max_nb_epochs=20)  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61       319\n",
      "           1       0.51      0.77      0.61       389\n",
      "           2       0.67      0.54      0.60       394\n",
      "           3       0.53      0.57      0.55       392\n",
      "           4       0.54      0.63      0.58       385\n",
      "           5       0.70      0.53      0.60       395\n",
      "           6       0.79      0.70      0.74       390\n",
      "           7       0.79      0.84      0.82       396\n",
      "           8       0.91      0.64      0.76       398\n",
      "           9       0.77      0.92      0.84       397\n",
      "          10       0.97      0.78      0.87       399\n",
      "          11       0.83      0.79      0.81       396\n",
      "          12       0.56      0.59      0.57       393\n",
      "          13       0.81      0.84      0.82       396\n",
      "          14       0.90      0.82      0.86       394\n",
      "          15       0.81      0.82      0.82       398\n",
      "          16       0.69      0.75      0.72       364\n",
      "          17       0.88      0.86      0.87       376\n",
      "          18       0.53      0.53      0.53       310\n",
      "          19       0.51      0.50      0.51       251\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      7532\n",
      "   macro avg       0.72      0.70      0.70      7532\n",
      "weighted avg       0.72      0.71      0.71      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        predicted = model.forward(torch.LongTensor(X_test_Glove))\n",
    "\n",
    "# get classification report\n",
    "predicted = predicted.detach().numpy()\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
