{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Convolutional Neural Networks (RCNN)\n",
    "Recurrent Convolutional Neural Networks (RCNN) is also used for text classification. The main idea of this technique is capturing contextual information with the recurrent structure and constructing the representation of text using a convolutional neural network. This architecture is a combination of RNN and CNN to use the advantages of both technique in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text Using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a data tokenizer\n",
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=179210, \\\n",
    "                       MAX_SEQUENCE_LENGTH=500):\n",
    "    '''\n",
    "    The function takes Train and Test datasets with text.\n",
    "    Converts them into tokens, and returns tokenized version of\n",
    "    both the sets, and the embedding matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : list with each item having a set of words that will be used\n",
    "    for training the model \n",
    "    X_test : list with each item having a set of words that will be used\n",
    "    for testing the model\n",
    "    MAX_NB_WORDS : Number of maximum words to be added in the tokenizer \n",
    "    vocabulary\n",
    "    MAX_SEQUENCE_LENGTH : Maximum length of sentences in the \n",
    "    '''\n",
    "    # set a random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "    \n",
    "    # concatenate train and text to build a combined vocabulary\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    \n",
    "    # initiate tokenizer\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    \n",
    "    # fit tokenizer on texts\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    \n",
    "    # build sequences\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    # dictionary for total vocabulary\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # pad sequences from left to make them of equal lengths\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # total unique words in vocab\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    # split tokenized text into train and test sets\n",
    "    indices = np.arange(text.shape[0])\n",
    "    text = text[indices]\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    \n",
    "    # create embedding using GLOVE\n",
    "    embeddings_index = {}\n",
    "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # print total words in embedding\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    # return train, test, vocabulary and embedding details\n",
    "    return (X_train, X_test, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model class using Pytorch Lightning\n",
    "\n",
    "Pytorch Lightning provides a standard wrapper to load data, define and train deep learning models.\n",
    "\n",
    "In this codeblock we define:\n",
    "1. Model\n",
    "2. Training/Validation/Test Steps\n",
    "3. Optimizer settings\n",
    "4. Train/Validation/Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "class CoolSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embedding_matrix, nclasses):\n",
    "        '''\n",
    "        Deep neural network with 5 layers of 512 fully connected nodes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: the dimensions of input layer\n",
    "        nclasses: the dimensions of output layer\n",
    "        dropout: the probability of dropping out.\n",
    "        '''\n",
    "        super(CoolSystem, self).__init__()\n",
    "        \n",
    "        self.nclasses = nclasses\n",
    "        \n",
    "        ## Embedding Layer, Add parameter \n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], \\\n",
    "                                      embedding_matrix.shape[1]) \n",
    "        et = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        self.embedding.weight = nn.Parameter(et)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(0.25)\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv1d(50, 256, kernel_size = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(256, 256, kernel_size = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(256, 256, kernel_size = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(256, 256, kernel_size = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "\n",
    "        self.RNNfeature1 = nn.Sequential(\n",
    "            nn.LSTM(256, 256,batch_first = True, dropout = 0.2))\n",
    "        self.RNNfeature2 = nn.Sequential(\n",
    "            nn.LSTM(256, 256,batch_first = True, dropout = 0.2))\n",
    "        self.RNNfeature3 = nn.Sequential(\n",
    "            nn.LSTM(256, 256,batch_first = True, dropout = 0.2))\n",
    "        self.RNNfeature4 = nn.Sequential(\n",
    "            nn.LSTM(256, 256,batch_first = True, dropout = 0.2))\n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, nclasses)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Passes the input through Deep neural network defined before.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: input\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.feature(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x, _ = self.RNNfeature1(x)\n",
    "        x, _ = self.RNNfeature2(x)\n",
    "        x, _ = self.RNNfeature3(x)\n",
    "        x, _ = self.RNNfeature4(x)\n",
    "        x = x[:,29,:]\n",
    "        x = self.feature1(x)\n",
    "                \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward\n",
    "        through network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward\n",
    "        through trained network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        '''\n",
    "        Takes and stacks validation loss.\n",
    "        Early stop can also be defined here\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Outputs: Output of validation step\n",
    "        '''\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Optimizer for the network\n",
    "\n",
    "        '''\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    @pl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        '''\n",
    "        Training data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_train_Glove),\\\n",
    "                                        torch.LongTensor(y_train)),\\\n",
    "               batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        '''\n",
    "        Validation data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_test_Glove),\\\n",
    "                                        torch.LongTensor(y_test)),\\\n",
    "               batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        '''\n",
    "        Test data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.LongTensor(X_test_Glove),\\\n",
    "                                        torch.LongTensor(y_test)),\\\n",
    "               batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text dataset (20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179209 unique tokens.\n",
      "Total 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Load test data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# make x and y\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# tokenize text and obtain embedding matrix\n",
    "X_train_Glove,X_test_Glove, embedding_matrix = loadData_Tokenizer(X_train,\\\n",
    "                                                                  X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model using Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False, used: False\n",
      "                 Name        Type   Params\n",
      "0           embedding   Embedding  8960500\n",
      "1   embedding_dropout   Dropout2d        0\n",
      "2             feature  Sequential   419840\n",
      "3           feature.0      Conv1d    25856\n",
      "4           feature.1        ReLU        0\n",
      "5           feature.2   MaxPool1d        0\n",
      "6           feature.3      Conv1d   131328\n",
      "7           feature.4        ReLU        0\n",
      "8           feature.5   MaxPool1d        0\n",
      "9           feature.6      Conv1d   131328\n",
      "10          feature.7        ReLU        0\n",
      "11          feature.8   MaxPool1d        0\n",
      "12          feature.9      Conv1d   131328\n",
      "13         feature.10        ReLU        0\n",
      "14         feature.11   MaxPool1d        0\n",
      "15        RNNfeature1  Sequential   526336\n",
      "16      RNNfeature1.0        LSTM   526336\n",
      "17        RNNfeature2  Sequential   526336\n",
      "18      RNNfeature2.0        LSTM   526336\n",
      "19        RNNfeature3  Sequential   526336\n",
      "20      RNNfeature3.0        LSTM   526336\n",
      "21        RNNfeature4  Sequential   526336\n",
      "22      RNNfeature4.0        LSTM   526336\n",
      "23           feature1  Sequential   283668\n",
      "24         feature1.0      Linear   263168\n",
      "25         feature1.1        ReLU        0\n",
      "26         feature1.2      Linear    20500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [09:32<00:00,  1.85s/it, avg_val_loss=1.36, batch_nb=88, epoch=14, loss=0.575]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = CoolSystem(embedding_matrix, 20)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer(max_nb_epochs=15)  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.28      0.35       319\n",
      "           1       0.51      0.46      0.48       389\n",
      "           2       0.61      0.57      0.59       394\n",
      "           3       0.39      0.45      0.42       392\n",
      "           4       0.27      0.38      0.31       385\n",
      "           5       0.55      0.29      0.38       395\n",
      "           6       0.50      0.78      0.61       390\n",
      "           7       0.86      0.63      0.73       396\n",
      "           8       0.71      0.79      0.75       398\n",
      "           9       0.81      0.80      0.81       397\n",
      "          10       0.80      0.96      0.87       399\n",
      "          11       0.93      0.70      0.80       396\n",
      "          12       0.58      0.44      0.50       393\n",
      "          13       0.85      0.76      0.80       396\n",
      "          14       0.86      0.80      0.83       394\n",
      "          15       0.55      0.88      0.68       398\n",
      "          16       0.67      0.68      0.68       364\n",
      "          17       0.95      0.74      0.83       376\n",
      "          18       0.42      0.53      0.47       310\n",
      "          19       0.34      0.26      0.29       251\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      7532\n",
      "   macro avg       0.63      0.61      0.61      7532\n",
      "weighted avg       0.64      0.62      0.62      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        predicted = model.forward(torch.LongTensor(X_test_Glove))\n",
    "\n",
    "# get classification report\n",
    "predicted = predicted.detach().numpy()\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
