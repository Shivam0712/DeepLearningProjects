{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "Deep Neural Networks architectures are designed to learn through multiple connections of layers where every single layer only receives a connection from previous and provides connections only to the next layer in the hidden part. The input layer is embedding vectors as shown in Figure below. The output layer neurons equal to the number of classes for multi-class classification and only one neuron for binary classification. Here, we have multi-class DNNs where the number of nodes in each layer as well as the number of layers are randomly assigned. The implementation of Deep Neural Network (DNN) is basically a discriminatively trained model that uses the standard back-propagation algorithm and sigmoid or ReLU as activation functions. The output layer for multi-class classification should use Softmax.\n",
    "\n",
    "\n",
    "<img src=\"./images/DeepNeuralNetwork.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to TF-IDF:\n",
    "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
    "    '''\n",
    "    In information retrieval, tf–idf or TFIDF, short for term frequency–\n",
    "    inverse document frequency, is a numerical statistic that is intended\n",
    "    to reflect how important a word is to a document in a collection or \n",
    "    corpus.[1] It is often used as a weighting factor in searches of \n",
    "    information retrieval, text mining, and user modeling. The tf–idf value\n",
    "    increases proportionally to the number of times a word appears in the \n",
    "    document and is offset by the number of documents in the corpus that \n",
    "    contain the word, which helps to adjust for the fact that some words \n",
    "    appear more frequently in general. tf–idf is one of the most popular \n",
    "    term-weighting schemes today; 83% of text-based recommender systems in\n",
    "    digital libraries use tf–idf.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : list with each item having a set of words that will be used\n",
    "    for training the model \n",
    "    X_test : list with each item having a set of words that will be used\n",
    "    for testing the model\n",
    "    MAX_NB_WORDS : Number of features in the vector\n",
    "    '''\n",
    "    # initiate vectorizer for obtaining fix dimensional vectors \n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    \n",
    "    # vectorize the train set\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    \n",
    "    # vectorize the test set\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    \n",
    "    # print number of features in the vector\n",
    "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
    "    \n",
    "    # return train and test set\n",
    "    return (X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model class using Pytorch Lightning\n",
    "\n",
    "Pytorch Lightning provides a standard wrapper to load data, define and train deep learning models.\n",
    "\n",
    "In this codeblock we define:\n",
    "1. Model\n",
    "2. Training/Validation/Test Steps\n",
    "3. Optimizer settings\n",
    "4. Train/Validation/Test Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "class CoolSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, shape, nclasses, dropout = 0.5):\n",
    "        '''\n",
    "        Deep neural network with 5 layers of 512 fully connected nodes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: the dimensions of input layer\n",
    "        nclasses: the dimensions of output layer\n",
    "        dropout: the probability of dropping out.\n",
    "        '''\n",
    "        super(CoolSystem, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.nclasses = nclasses\n",
    "        self.dropout = dropout\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(512, nclasses))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Passes the input through Deep neural network defined before.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: input\n",
    "        '''\n",
    "        x = self.feature(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward through network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        '''\n",
    "        Training step, takes the training batch and pass it forward through trained network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: input\n",
    "        batch_nb: batch number\n",
    "        '''\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return {'val_loss': criterion(y_hat, y)}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        '''\n",
    "        Takes and stacks validation loss.\n",
    "        Early stop can also be defined here\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Outputs: Output of validation step\n",
    "        '''\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Optimizer for the network\n",
    "\n",
    "        '''\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    @pl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        '''\n",
    "        Training data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.Tensor(X_train_tfidf), torch.LongTensor(y_train)), batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        '''\n",
    "        Validation data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.Tensor(X_test_tfidf), torch.LongTensor(y_test)), batch_size=128)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        '''\n",
    "        Test data loader, takes input directly from global environment\n",
    "        Preprocessing can also be defined here.\n",
    "        \n",
    "        '''\n",
    "        return\\\n",
    "    DataLoader(data_utils.TensorDataset(torch.Tensor(X_test_tfidf), torch.LongTensor(y_test)), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text dataset (20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 75000 features\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# load test data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# make x and y\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# Convert the text TFIDF\n",
    "X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model using Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False, used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Name        Type    Params\n",
      "0      feature  Sequential  39461396\n",
      "1    feature.0      Linear  38400512\n",
      "2    feature.1        ReLU         0\n",
      "3    feature.2     Dropout         0\n",
      "4    feature.3      Linear    262656\n",
      "5    feature.4        ReLU         0\n",
      "6    feature.5     Dropout         0\n",
      "7    feature.6      Linear    262656\n",
      "8    feature.7        ReLU         0\n",
      "9    feature.8     Dropout         0\n",
      "10   feature.9      Linear    262656\n",
      "11  feature.10        ReLU         0\n",
      "12  feature.11     Dropout         0\n",
      "13  feature.12      Linear    262656\n",
      "14  feature.13        ReLU         0\n",
      "15  feature.14     Dropout         0\n",
      "16  feature.15      Linear     10260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [02:09<00:00,  5.55it/s, avg_val_loss=1.33, batch_nb=88, epoch=9, loss=0.083]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = CoolSystem(75000, 20)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer(max_nb_epochs=10)  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.74       319\n",
      "           1       0.67      0.62      0.64       389\n",
      "           2       0.74      0.55      0.63       394\n",
      "           3       0.55      0.70      0.62       392\n",
      "           4       0.57      0.80      0.67       385\n",
      "           5       0.84      0.70      0.76       395\n",
      "           6       0.88      0.82      0.85       390\n",
      "           7       0.83      0.70      0.76       396\n",
      "           8       0.98      0.91      0.94       398\n",
      "           9       0.93      0.87      0.90       397\n",
      "          10       0.99      0.90      0.94       399\n",
      "          11       0.98      0.81      0.88       396\n",
      "          12       0.51      0.79      0.62       393\n",
      "          13       0.93      0.65      0.77       396\n",
      "          14       0.79      0.91      0.84       394\n",
      "          15       0.91      0.86      0.89       398\n",
      "          16       0.77      0.87      0.81       364\n",
      "          17       0.98      0.80      0.88       376\n",
      "          18       0.56      0.59      0.58       310\n",
      "          19       0.56      0.69      0.62       251\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      7532\n",
      "   macro avg       0.79      0.76      0.77      7532\n",
      "weighted avg       0.79      0.77      0.77      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get prediction\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        predicted = model.forward(torch.Tensor(X_test_tfidf))\n",
    "\n",
    "# get classification report\n",
    "predicted = predicted.detach().numpy()\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
