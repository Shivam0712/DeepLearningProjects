{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general modules\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import random\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch modules\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.nn.functional  as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,utils\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define requisite function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findcp():\n",
    "    '''\n",
    "    Function to find checkpoint number of saved model.\n",
    "    '''\n",
    "    cp = [0]\n",
    "    for i in os.listdir(path+'/data/models/transfer_ad/'):\n",
    "        if '_model.pth.tar' in i:\n",
    "            cp.append(int(i.split('_')[0]))\n",
    "    return str(max(cp)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename= path+'/data/models/transfer_ad/checkpoint.pth.tar'):\n",
    "    '''\n",
    "    Function to save checkpoint of learned model.\n",
    "    '''\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    '''\n",
    "    Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    '''\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path\n",
    "path = os.getcwd()\n",
    "\n",
    "# define train data paths\n",
    "train_dir = path + '/data/data_Aug/train/'\n",
    "\n",
    "# define validation data paths\n",
    "val_dir = path + '/data/data_Aug/val/'\n",
    "\n",
    "# define test data path\n",
    "test_dir = path + '/data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    't':transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets from folders\n",
    "data = {\n",
    "    'train':\n",
    "    datasets.ImageFolder(root=train_dir, transform=image_transforms['t']),\n",
    "    'val':\n",
    "    datasets.ImageFolder(root=val_dir, transform=image_transforms['t']),\n",
    "    'test':\n",
    "    ImageFolderWithPaths(root=test_dir, transform=image_transforms['t'])\n",
    "}\n",
    "\n",
    "# Dataloader iterators, make sure to shuffle\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=128, shuffle=True, num_workers=2),\n",
    "    'val': DataLoader(data['val'], batch_size=128, shuffle=True, num_workers=2),\n",
    "    'test': DataLoader(data['test'], batch_size=128, num_workers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of classses\n",
    "n_class = len(data['val'].classes)\n",
    "\n",
    "# define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_conv= models.inception_v3(pretrained = True)\n",
    "\n",
    "# add final pooling layer\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Sequential(\n",
    "                   nn.Linear(num_ftrs, 512),\n",
    "                   nn.ReLU(inplace=True),\n",
    "                   nn.Linear(512, n_class))\n",
    "\n",
    "# multiple GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_conv = nn.DataParallel(model_conv)\n",
    "\n",
    "# send model to device\n",
    "model_conv.to(device)\n",
    "\n",
    "print('Model Defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save models\n",
    "try:\n",
    "    os.listdir(path+'/data/models/transfer_ad/')\n",
    "except:\n",
    "    os.mkdir(path+'/data/models/transfer_ad/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels in a dataframe\n",
    "df_label = pd.DataFrame()\n",
    "df_label['category'] = data['train'].classes\n",
    "df_label['category'] = df_label['category'].astype(int)\n",
    "df_label['c'] = df_label.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define criterion and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_conv.parameters())\n",
    "\n",
    "# define number of epochs\n",
    "num_epochs = 25\n",
    "\n",
    "# define min loss \n",
    "min_loss = 10000\n",
    "\n",
    "# train and validate\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # print the initiation of epoch\n",
    "    print('{} : Epoch {}/{}'.format(dt.datetime.now(),epoch+1, num_epochs))\n",
    "    print('-' * 20)\n",
    "    \n",
    "    \n",
    "    # run training and validation phase \n",
    "    for phase in ['train', 'val']:\n",
    "        \n",
    "        # choose to train or eval model\n",
    "        if phase == 'train':\n",
    "            model_conv.train()\n",
    "        else:\n",
    "            model_conv.eval()\n",
    "\n",
    "        # initiate variables\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        batch = 0\n",
    "        \n",
    "        # run batch training/validation\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            \n",
    "            # inputs\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # outputs\n",
    "            if phase == 'train':\n",
    "                outputs, aux = model_conv(inputs)\n",
    "            else:\n",
    "                outputs = model_conv(inputs)\n",
    "            \n",
    "            # loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # update gradient\n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # get prediction\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # append loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            \n",
    "            # print batch statistics\n",
    "            batch = batch + 1\n",
    "            if batch % 10 == 0:\n",
    "                print('{} {} Batch:{} loss: {:.4f}, acc: {:.4f}'.format(dt.datetime.now(),\n",
    "                                                                        phase,\n",
    "                                                                        batch,\n",
    "                                                                        running_loss/(batch * 128),\n",
    "                                                                        running_corrects.double()/(batch * 128)))\n",
    "        \n",
    "            \n",
    "        # get epoch statistics\n",
    "        epoch_loss = running_loss / len(data[phase])\n",
    "        epoch_acc = running_corrects.double() / len(data[phase])\n",
    "\n",
    "        print('{} Epoch:{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                             epoch,\n",
    "                                                             epoch_loss,\n",
    "                                                             epoch_acc))\n",
    "        \n",
    "        # save best model\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                # save model\n",
    "                save_checkpoint({\n",
    "                                     'epoch': epoch + 1,\n",
    "                                     'arch': 'model',\n",
    "                                     'state_dict': model_conv.state_dict(),\n",
    "                                     'current_loss': epoch_loss,\n",
    "                                     'current_acc': epoch_acc, \n",
    "                                     'optimizer' : optimizer.state_dict(),\n",
    "                                 }, filename= path+'/data/models/transfer_ad/'+ findcp() + '_model.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get Prediction\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "for inputs, labels, path in dataloaders['test']:\n",
    "\n",
    "    # inputs\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # outputs\n",
    "    outputs = model_conv(inputs)\n",
    "    \n",
    "    # prediction\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    temp_output = pd.DataFrame()\n",
    "    temp_output['path'] = path\n",
    "    temp_output['c'] = preds.to('cpu')\n",
    "    df_test = df_test.append(temp_output)\n",
    "\n",
    "# get image id from path\n",
    "df_test['image_id'] = df_test['path'].apply(lambda x: int(x.split('/')[-1].split('.jpg')[0]))\n",
    "del df_test['path']\n",
    "\n",
    "# map the the labels obtained from model using df_label\n",
    "df_test = df_test.merge(df_label, on = 'c', how ='left')\n",
    "del df_test['c']\n",
    "\n",
    "# sort\n",
    "df_test.sort_values(by = 'image_id')\n",
    "\n",
    "# test head\n",
    "df_test.head()\n",
    "\n",
    "# save output\n",
    "df_test.to_csv(path + '/data/outputs/tl_ad_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
